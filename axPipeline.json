{"abstract": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "title": "[1608.07094] A Novel Term_Class Relevance Measure for Text Categorization", "url": "http://arxiv.org/abs/1608.07094", "authors": ["D S Guru", "Mahamad Suhil"], "num": 5, "sec_subject": "cs.CL", "order_index": 7465}
{"abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "title": "[1608.07187] Semantics derived automatically from language corpora necessarily contain human biases", "url": "http://arxiv.org/abs/1608.07187", "authors": ["Aylin Caliskan-Islam", "Joanna J. Bryson", "Arvind Narayanan"], "num": 4, "sec_subject": "cs.CL", "order_index": 7464}
{"abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.", "title": "[1608.07115] Aligning Packed Dependency Trees: a theory of composition for distributional semantics", "url": "http://arxiv.org/abs/1608.07115", "authors": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "num": 1, "sec_subject": "cs.CL", "order_index": 7461}
{"abstract": "Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is two-fold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance.", "title": "[1608.07249] Benchmarking State-of-the-Art Deep Learning Software Tools", "url": "http://arxiv.org/abs/1608.07249", "authors": ["Shaohuai Shi", "Qiang Wang", "Pengfei Xu", "Xiaowen Chu"], "num": 7, "sec_subject": "cs.LG", "order_index": 8317}
{"abstract": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and non-convex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.", "title": "[1608.06984] Learning Human Search Strategies from a Crowdsourcing Game", "url": "http://arxiv.org/abs/1608.06984", "authors": ["Thurston Sexton", "Max Yi Ren"], "num": 6, "sec_subject": "cs.LG", "order_index": 8316}
{"abstract": "The problem of recommending tours to travellers is an important and broadly studied area. Suggested solutions include various approaches of points-of-interest (POI) recommendation and route planning. We consider the task of recommending a sequence of POIs, that simultaneously uses information about POIs and routes. Our approach unifies the treatment of various sources of information by representing them as features in machine learning algorithms, enabling us to learn from past behaviour. Information about POIs are used to learn a POI ranking model that accounts for the start and end points of tours. Data about previous trajectories are used for learning transition patterns between POIs that enable us to recommend probable routes. In addition, a probabilistic model is proposed to combine the results of POI ranking and the POI to POI transitions. We propose a new F$_1$ score on pairs of POIs that capture the order of visits. Empirical results show that our approach improves on recent methods, and demonstrate that combining points and routes enables better trajectory recommendations.", "title": "[1608.07051] Learning Points and Routes to Recommend Trajectories", "url": "http://arxiv.org/abs/1608.07051", "authors": ["Dawei Chen", "Cheng Soon Ong", "Lexing Xie"], "num": 4, "sec_subject": "cs.LG", "order_index": 8314}
{"abstract": "In many practical applications of learning algorithms, unlabeled data is cheap and abundant whereas labeled data is expensive. Active learning algorithms developed to achieve better performance with lower cost. Usually Representativeness and Informativeness are used in active learning algoirthms. Advanced recent active learning methods consider both of these criteria. Despite its vast literature, very few active learning methods consider noisy instances, i.e. label noisy and outlier instances. Also, these methods didn't consider accuracy in computing representativeness and informativeness. Based on the idea that inaccuracy in these measures and not taking noisy instances into consideration are two sides of a coin and are inherently related, a new loss function is proposed. This new loss function helps to decrease the effect of noisy instances while at the same time, reduces bias. We defined \"instance complexity\" as a new notion of complexity for instances of a learning problem. It is proved that noisy instances in the data if any, are the ones with maximum instance complexity. Based on this loss function which has two functions for classifying ordinary and noisy instances, a new classifier, named \"Simple-Complex Classifier\" is proposed. In this classifier there are a simple and a complex function, with the complex function responsible for selecting noisy instances. The resulting optimization problem for both learning and active learning is highly non-convex and very challenging. In order to solve it, a convex relaxation is proposed.", "title": "[1608.07159] Active Robust Learning", "url": "http://arxiv.org/abs/1608.07159", "authors": ["Hossein Ghafarian", "Hadi Sadoghi Yazdi"], "num": 3, "sec_subject": "cs.LG", "order_index": 8313}
{"abstract": "A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following $n$-dimensional quadratic minimization problem in constant time, which is independent of $n$: $z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle + n\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle + n\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$ is a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our theoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such that the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with probability $1-\\delta$. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.", "title": "[1608.07179] Minimizing Quadratic Functions in Constant Time", "url": "http://arxiv.org/abs/1608.07179", "authors": ["Kohei Hayashi", "Yuichi Yoshida"], "num": 2, "sec_subject": "cs.LG", "order_index": 8312}
{"abstract": "Genome-wide association studies (GWAS) offer new opportunities to identify genetic risk factors for Alzheimer's disease (AD). Recently, collaborative efforts across different institutions emerged that enhance the power of many existing techniques on individual institution data. However, a major barrier to collaborative studies of GWAS is that many institutions need to preserve individual data privacy. To address this challenge, we propose a novel distributed framework, termed Local Query Model (LQM) to detect risk SNPs for AD across multiple research institutions. To accelerate the learning process, we propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening rule to identify irrelevant features and remove them from the optimization. To the best of our knowledge, this is the first successful run of the computationally intensive model selection procedure to learn a consistent model across different institutions without compromising their privacy while ranking the SNPs that may collectively affect AD. Empirical studies are conducted on 809 subjects with 5.9 million SNP features which are distributed across three individual institutions. D-EDPP achieved a 66-fold speed-up by effectively identifying irrelevant features.", "title": "[1608.07251] Large-scale Collaborative Imaging Genetics Studies of Risk Genetic Factors for Alzheimer's Disease Across Multiple Institutions", "url": "http://arxiv.org/abs/1608.07251", "authors": ["Qingyang Li", "Tao Yang", "Liang Zhan", "Derrek Paul Hibar", "Neda Jahanshad", "Yalin Wang", "Jieping Ye", "Paul M. Thompson", "Jie Wang"], "num": 1, "sec_subject": "cs.LG", "order_index": 8311}
{"abstract": "Incremental clustering approaches have been proposed for handling large data when given data set is too large to be stored. The key idea of these approaches is to find representatives to represent each cluster in each data chunk and final data analysis is carried out based on those identified representatives from all the chunks. However, most of the incremental approaches are used for single view data. As large multi-view data generated from multiple sources becomes prevalent nowadays, there is a need for incremental clustering approaches to handle both large and multi-view data. In this paper we propose a new incremental clustering approach called incremental minimax optimization based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In IminimaxFCM, representatives with multiple views are identified to represent each cluster by integrating multiple complementary views using minimax optimization. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed IminimaxFCM are provided. Experimental studies on several real world multi-view data sets have been conducted. We observed that IminimaxFCM outperforms related incremental fuzzy clustering in terms of clustering accuracy, demonstrating the great potential of IminimaxFCM for large multi-view data analysis.", "title": "[1608.07001] Incremental Minimax Optimization based Fuzzy Clustering for Large Multi-view Data", "url": "http://arxiv.org/abs/1608.07001", "authors": ["Yangtao Wang", "Lihui Chen", "Xiaoli Li"], "num": 10, "sec_subject": "cs.LG", "order_index": 8320}
{"abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).", "title": "[1608.06993] Densely Connected Convolutional Networks", "url": "http://arxiv.org/abs/1608.06993", "authors": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "num": 11, "sec_subject": "cs.LG", "order_index": 8321}
