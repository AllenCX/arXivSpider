{"abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).", "sec_subject": "cs.LG", "num": 11, "url": "http://arxiv.org/abs/1608.06993", "authors": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "title": "[1608.06993] Densely Connected Convolutional Networks"}
{"abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "sec_subject": "cs.LG", "num": 8, "url": "http://arxiv.org/abs/1608.07187", "authors": ["Aylin Caliskan-Islam", "Joanna J. Bryson", "Arvind Narayanan"], "title": "[1608.07187] Semantics derived automatically from language corpora necessarily contain human biases"}
{"abstract": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and non-convex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.", "sec_subject": "cs.LG", "num": 6, "url": "http://arxiv.org/abs/1608.06984", "authors": ["Thurston Sexton", "Max Yi Ren"], "title": "[1608.06984] Learning Human Search Strategies from a Crowdsourcing Game"}
{"abstract": "Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is two-fold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance.", "sec_subject": "cs.LG", "num": 7, "url": "http://arxiv.org/abs/1608.07249", "authors": ["Shaohuai Shi", "Qiang Wang", "Pengfei Xu", "Xiaowen Chu"], "title": "[1608.07249] Benchmarking State-of-the-Art Deep Learning Software Tools"}
{"abstract": "A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following $n$-dimensional quadratic minimization problem in constant time, which is independent of $n$: $z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle + n\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle + n\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$ is a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our theoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such that the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with probability $1-\\delta$. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.", "sec_subject": "cs.LG", "num": 2, "url": "http://arxiv.org/abs/1608.07179", "authors": ["Kohei Hayashi", "Yuichi Yoshida"], "title": "[1608.07179] Minimizing Quadratic Functions in Constant Time"}
{"abstract": "Genome-wide association studies (GWAS) offer new opportunities to identify genetic risk factors for Alzheimer's disease (AD). Recently, collaborative efforts across different institutions emerged that enhance the power of many existing techniques on individual institution data. However, a major barrier to collaborative studies of GWAS is that many institutions need to preserve individual data privacy. To address this challenge, we propose a novel distributed framework, termed Local Query Model (LQM) to detect risk SNPs for AD across multiple research institutions. To accelerate the learning process, we propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening rule to identify irrelevant features and remove them from the optimization. To the best of our knowledge, this is the first successful run of the computationally intensive model selection procedure to learn a consistent model across different institutions without compromising their privacy while ranking the SNPs that may collectively affect AD. Empirical studies are conducted on 809 subjects with 5.9 million SNP features which are distributed across three individual institutions. D-EDPP achieved a 66-fold speed-up by effectively identifying irrelevant features.", "sec_subject": "cs.LG", "num": 1, "url": "http://arxiv.org/abs/1608.07251", "authors": ["Qingyang Li", "Tao Yang", "Liang Zhan", "Derrek Paul Hibar", "Neda Jahanshad", "Yalin Wang", "Jieping Ye", "Paul M. Thompson", "Jie Wang"], "title": "[1608.07251] Large-scale Collaborative Imaging Genetics Studies of Risk Genetic Factors for Alzheimer's Disease Across Multiple Institutions"}
{"abstract": "Random Projection (RP) technique has been widely applied in many scenarios because it can reduce high-dimensional features into low-dimensional space within short time and meet the need of real-time analysis of massive data. There is an urgent need of dimensionality reduction with fast increase of big genomics data. However, the performance of RP is usually lower. We attempt to improve classification accuracy of RP through combining other reduction dimension methods such as Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Feature Selection (FS). We compared classification accuracy and running time of different combination methods on three microarray datasets and a simulation dataset. Experimental results show a remarkable improvement of 14.77% in classification accuracy of FS followed by RP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield a more discriminative subspace with an increase of 13.65% on classification accuracy on the same dataset. FS followed by RP outperforms other combination methods in classification accuracy on most of the datasets.", "sec_subject": "cs.LG", "num": 5, "url": "http://arxiv.org/abs/1608.07019", "authors": ["Haozhe Xie", "Jie Li", "Qiaosheng Zhang", "Yadong Wang"], "title": "[1608.07019] Comparison among dimensionality reduction techniques based on Random Projection for cancer classification"}
{"abstract": "In many practical applications of learning algorithms, unlabeled data is cheap and abundant whereas labeled data is expensive. Active learning algorithms developed to achieve better performance with lower cost. Usually Representativeness and Informativeness are used in active learning algoirthms. Advanced recent active learning methods consider both of these criteria. Despite its vast literature, very few active learning methods consider noisy instances, i.e. label noisy and outlier instances. Also, these methods didn't consider accuracy in computing representativeness and informativeness. Based on the idea that inaccuracy in these measures and not taking noisy instances into consideration are two sides of a coin and are inherently related, a new loss function is proposed. This new loss function helps to decrease the effect of noisy instances while at the same time, reduces bias. We defined \"instance complexity\" as a new notion of complexity for instances of a learning problem. It is proved that noisy instances in the data if any, are the ones with maximum instance complexity. Based on this loss function which has two functions for classifying ordinary and noisy instances, a new classifier, named \"Simple-Complex Classifier\" is proposed. In this classifier there are a simple and a complex function, with the complex function responsible for selecting noisy instances. The resulting optimization problem for both learning and active learning is highly non-convex and very challenging. In order to solve it, a convex relaxation is proposed.", "sec_subject": "cs.LG", "num": 3, "url": "http://arxiv.org/abs/1608.07159", "authors": ["Hossein Ghafarian", "Hadi Sadoghi Yazdi"], "title": "[1608.07159] Active Robust Learning"}
{"abstract": "The problem of recommending tours to travellers is an important and broadly studied area. Suggested solutions include various approaches of points-of-interest (POI) recommendation and route planning. We consider the task of recommending a sequence of POIs, that simultaneously uses information about POIs and routes. Our approach unifies the treatment of various sources of information by representing them as features in machine learning algorithms, enabling us to learn from past behaviour. Information about POIs are used to learn a POI ranking model that accounts for the start and end points of tours. Data about previous trajectories are used for learning transition patterns between POIs that enable us to recommend probable routes. In addition, a probabilistic model is proposed to combine the results of POI ranking and the POI to POI transitions. We propose a new F$_1$ score on pairs of POIs that capture the order of visits. Empirical results show that our approach improves on recent methods, and demonstrate that combining points and routes enables better trajectory recommendations.", "sec_subject": "cs.LG", "num": 4, "url": "http://arxiv.org/abs/1608.07051", "authors": ["Dawei Chen", "Cheng Soon Ong", "Lexing Xie"], "title": "[1608.07051] Learning Points and Routes to Recommend Trajectories"}
{"abstract": "Incremental clustering approaches have been proposed for handling large data when given data set is too large to be stored. The key idea of these approaches is to find representatives to represent each cluster in each data chunk and final data analysis is carried out based on those identified representatives from all the chunks. However, most of the incremental approaches are used for single view data. As large multi-view data generated from multiple sources becomes prevalent nowadays, there is a need for incremental clustering approaches to handle both large and multi-view data. In this paper we propose a new incremental clustering approach called incremental minimax optimization based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In IminimaxFCM, representatives with multiple views are identified to represent each cluster by integrating multiple complementary views using minimax optimization. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed IminimaxFCM are provided. Experimental studies on several real world multi-view data sets have been conducted. We observed that IminimaxFCM outperforms related incremental fuzzy clustering in terms of clustering accuracy, demonstrating the great potential of IminimaxFCM for large multi-view data analysis.", "sec_subject": "cs.LG", "num": 10, "url": "http://arxiv.org/abs/1608.07001", "authors": ["Yangtao Wang", "Lihui Chen", "Xiaoli Li"], "title": "[1608.07001] Incremental Minimax Optimization based Fuzzy Clustering for Large Multi-view Data"}
{"abstract": "Multi-view data clustering refers to categorizing a data set by making good use of related information from multiple representations of the data. It becomes important nowadays because more and more data can be collected in a variety of ways, in different settings and from different sources, so each data set can be represented by different sets of features to form different views of it. Many approaches have been proposed to improve clustering performance by exploring and integrating heterogeneous information underlying different views. In this paper, we propose a new multi-view fuzzy clustering approach called MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In MinimaxFCM the consensus clustering results are generated based on minimax optimization in which the maximum disagreements of different weighted views are minimized. Moreover, the weight of each view can be learned automatically in the clustering process. In addition, there is only one parameter to be set besides the fuzzifier. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed MinimaxFCM are provided here. Experimental studies on nine multi-view data sets including real world image and document data sets have been conducted. We observed that MinimaxFCM outperforms related multi-view clustering approaches in terms of clustering accuracy, demonstrating the great potential of MinimaxFCM for multi-view data analysis.", "sec_subject": "cs.LG", "num": 9, "url": "http://arxiv.org/abs/1608.07005", "authors": ["Yangtao Wang", "Lihui Chen"], "title": "[1608.07005] Multi-View Fuzzy Clustering with Minimax Optimization for Effective Clustering of Data from Multiple Sources"}
{"abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.", "sec_subject": "cs.CL", "num": 1, "url": "http://arxiv.org/abs/1608.07115", "authors": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "title": "[1608.07115] Aligning Packed Dependency Trees: a theory of composition for distributional semantics"}
{"abstract": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "sec_subject": "cs.CL", "num": 5, "url": "http://arxiv.org/abs/1608.07094", "authors": ["D S Guru", "Mahamad Suhil"], "title": "[1608.07094] A Novel Term_Class Relevance Measure for Text Categorization"}
{"abstract": "We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.", "sec_subject": "cs.CL", "num": 3, "url": "http://arxiv.org/abs/1608.07253", "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "title": "[1608.07253] Learning Latent Vector Spaces for Product Search"}
{"abstract": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "sec_subject": "cs.CL", "num": 2, "url": "http://arxiv.org/abs/1608.07076", "authors": ["Ond\u0159ej Du\u0161ek", "Filip Jur\u010d\u00ed\u010dek"], "title": "[1608.07076] A Context-aware Natural Language Generator for Dialogue Systems"}
