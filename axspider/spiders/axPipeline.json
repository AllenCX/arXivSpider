{"title": "[1603.05953] Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "sec_subject": "cs.LG", "num": 7, "authors": ["Zeyuan Allen-Zhu"], "order_index": 8317, "abstract": "The main ingredient behind our result is $\\textit{Katyusha momentum}$, a novel \"negative momentum on top of momentum\" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug.", "url": "http://arxiv.org/abs/1603.05953"}
{"title": "[1608.07441] Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "sec_subject": "cs.LG", "num": 3, "authors": ["Maxime Bucher", "St\u00e9phane Herbin", "Fr\u00e9d\u00e9ric Jurie"], "order_index": 8313, "abstract": "Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets.", "url": "http://arxiv.org/abs/1608.07441"}
{"title": "[1608.07400] Collaborative Filtering with Recurrent Neural Networks", "sec_subject": "cs.LG", "num": 5, "authors": ["Robin Devooght", "Hugues Bersini"], "order_index": 8315, "abstract": "We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.", "url": "http://arxiv.org/abs/1608.07400"}
{"title": "[1608.07502] Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events", "sec_subject": "cs.LG", "num": 2, "authors": ["Ting Chen", "Lu-An Tang", "Yizhou Sun", "Zhengzhang Chen", "Kai Zhang"], "order_index": 8312, "abstract": "Anomaly detection plays an important role in modern data-driven security applications, such as detecting suspicious access to a socket from a process. In many cases, such events can be described as a collection of categorical values that are considered as entities of different types, which we call heterogeneous categorical events. Due to the lack of intrinsic distance measures among entities, and the exponentially large event space, most existing work relies heavily on heuristics to calculate abnormal scores for events. Different from previous work, we propose a principled and unified probabilistic model APE (Anomaly detection via Probabilistic pairwise interaction and Entity embedding) that directly models the likelihood of events. In this model, we embed entities into a common latent space using their observed co-occurrence in different events. More specifically, we first model the compatibility of each pair of entities according to their embeddings. Then we utilize the weighted pairwise interactions of different entity types to define the event probability. Using Noise-Contrastive Estimation with \"context-dependent\" noise distribution, our model can be learned efficiently regardless of the large event space. Experimental results on real enterprise surveillance data show that our methods can accurately detect abnormal events compared to other state-of-the-art abnormal detection techniques.", "url": "http://arxiv.org/abs/1608.07502"}
{"title": "[1608.07253] Learning Latent Vector Spaces for Product Search", "sec_subject": "cs.CL", "num": 3, "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "order_index": 7463, "abstract": "We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.", "url": "http://arxiv.org/abs/1608.07253"}
{"title": "[1608.07115] Aligning Packed Dependency Trees: a theory of composition for distributional semantics", "sec_subject": "cs.CL", "num": 1, "authors": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "order_index": 7461, "abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.", "url": "http://arxiv.org/abs/1608.07115"}
{"title": "[1608.07187] Semantics derived automatically from language corpora necessarily contain human biases", "sec_subject": "cs.CL", "num": 4, "authors": ["Aylin Caliskan-Islam", "Joanna J. Bryson", "Arvind Narayanan"], "order_index": 7464, "abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "url": "http://arxiv.org/abs/1608.07187"}
{"title": "[1608.07094] A Novel Term_Class Relevance Measure for Text Categorization", "sec_subject": "cs.CL", "num": 5, "authors": ["D S Guru", "Mahamad Suhil"], "order_index": 7465, "abstract": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "url": "http://arxiv.org/abs/1608.07094"}
{"title": "[1608.07076] A Context-aware Natural Language Generator for Dialogue Systems", "sec_subject": "cs.CL", "num": 2, "authors": ["Ond\u0159ej Du\u0161ek", "Filip Jur\u010d\u00ed\u010dek"], "order_index": 7462, "abstract": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic me{"order_index": 8314, "num": 4, "authors": ["Farshad Lahouti", "Babak Hassibi"], "abstract": "Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed $k$-ary incidence coding and study optimized query pricing in this setting.", "url": "http://arxiv.org/abs/1608.07328", "title": "[1608.07328] Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing", "sec_subject": "cs.LG"}
{"order_index": 7461, "num": 1, "authors": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.", "url": "http://arxiv.org/abs/1608.07115", "title": "[1608.07115] Aligning Packed Dependency Trees: a theory of composition for distributional semantics", "sec_subject": "cs.CL"}
{"order_index": 8317, "num": 7, "authors": ["Zeyuan Allen-Zhu"], "abstract": "The main ingredient behind our result is $\\textit{Katyusha momentum}$, a novel \"negative momentum on top of momentum\" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug.", "url": "http://arxiv.org/abs/1603.05953", "title": "[1603.05953] Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "sec_subject": "cs.LG"}
xamine the scheme's convergence speed and we show that if the game admits a strict equilibrium and the players' mirror maps are surjective, then, with high probability, the process converges to equilibrium in a finite number of steps, no matter the level of uncertainty.", "url": "http://arxiv.org/abs/1608.07310"}
{"title": "[1608.07536] Leveraging over intact priors for boosting control and dexterity of prosthetic hands by amputees", "sec_subject": "cs.LG", "num": 1, "authors": ["Valentina Gregori", "Barbara Caputo"], "order_index": 8311, "abstract": "Non-invasive myoelectric prostheses require a long training time to obtain satisfactory control dexterity. These training times could possibly be reduced by leveraging over training efforts by previous subjects. So-called domain adaptation algorithms formalize this strategy and have indeed been shown to significantly reduce the amount of required training data for intact subjects for myoelectric movements classification. It is not clear, however, whether these results extend also to amputees and, if so, whether prior information from amputees and intact subjects is equally useful. To overcome this problem, we evaluated several domain adaptation algorithms on data coming from both amputees and intact subjects. Our findings indicate that: (1) the use of previous experience from other subjects allows us to reduce the training time by about an order of magnitude; (2) this improvement holds regardless of whether an amputee exploits previous information from other amputees or from intact subjects.", "url": "http://arxiv.org/abs/1608.07536"}
{"title": "[1608.07365] Scalable Compression of Deep Neural Networks", "sec_subject": "cs.CV", "num": 6, "authors": ["Xing Wang", "Jie Liang"], "order_index": 7566, "abstract": "Deep neural networks generally involve some layers with mil- lions of parameters, making them difficult to be deployed and updated on devices with limited resources such as mobile phones and other smart embedded systems. In this paper, we propose a scalable representation of the network parameters, so that different applications can select the most suitable bit rate of the network based on their own storage constraints. Moreover, when a device needs to upgrade to a high-rate network, the existing low-rate network can be reused, and only some incremental data are needed to be downloaded. We first hierarchically quantize the weights of a pre-trained deep neural network to enforce weight sharing. Next, we adaptively select the bits assigned to each layer given the total bit budget. After that, we retrain the network to fine-tune the quantized centroids. Experimental results show that our method can achieve scalable compression with graceful degradation in the performance.", "url": "http://arxiv.org/abs/1608.07365"}
{"title": "[1608.07433] Mean Deviation Similarity Index: Efficient and Reliable Full-Reference Image Quality Evaluator", "sec_subject": "cs.CV", "num": 4, "authors": ["Hossein Ziaei Nafchi", "Atena Shahkolaei", "Rachid Hedjam", "Mohamed Cheriet"], "order_index": 7564, "abstract": "", "url": "http://arxiv.org/abs/1608.07433"}
{"title": "[1608.07454] Fine Hand Segmentation using Convolutional Neural Networks", "sec_subject": "cs.CV", "num": 2, "authors": ["Tadej Vodopivec", "Vincent Lepetit", "Peter Peer"], "order_index": 7562, "abstract": "We propose a method for extracting very accurate masks of hands in egocentric views. Our method is based on a novel Deep Learning architecture: In contrast with current Deep Learning methods, we do not use upscaling layers applied to a low-dimensional representation of the input image. Instead, we extract features with convolutional layers and map them directly to a segmentation mask with a fully connected layer. We show that this approach, when applied in a multi-scale fashion, is both accurate and efficient enough for real-time. We demonstrate it on a new dataset made of images captured in various environments, from the outdoors to offices.", "url": "http://arxiv.org/abs/1608.07454"}
{"title": "[1608.07444] Who Leads the Clothing Fashion: Style, Color, or Texture? A Computational Study", "sec_subject": "cs.CV", "num": 3, "authors": ["Qin Zou", "Zheng Zhang", "Qian Wang", "Qingquan Li", "Long Chen", "Song Wang"], "order_index": 7563, "abstract": "It is well known that clothing fashion is a distinctive and often habitual trend in the style in which a person dresses. Clothing fashions are usually expressed with visual stimuli such as style, color, and texture. However, it is not clear which visual stimulus places higher/lower influence on the updating of clothing fashion. In this study, computer vision and machine learning techniques are employed to analyze the influence of different visual stimuli on clothing-fashion updates. Specifically, a classification-based model is proposed to quantify the influence of different visual stimuli, in which each visual stimulus's influence is quantified by its corresponding accuracy in fashion classification. Experimental results demonstrate that, on clothing-fashion updates, the style holds a higher influence than the color, and the color holds a higher influence than the texture.", "url": "http://arxiv.org/abs/1608.07444"}
{"title": "[1608.07470] A Fast Ellipse Detector Using Projective Invariant Pruning", "sec_subject": "cs.CV", "num": 1, "authors": ["Qi Jia", "Xin Fan", "Zhongxuan Luo", "Lianbo Song", "Tie Qiu"], "order_index": 7561, "abstract": "Detecting elliptical objects from an image is a central task in robot navigation and industrial diagnosis where the detection time is always a critical issue. Existing methods are hardly applicable to these real-time scenarios of limited hardware resource due to the huge number of fragment candidates (edges or arcs) for fitting ellipse equations. In this paper, we present a fast algorithm detecting ellipses with high accuracy. The algorithm leverage a newly developed projective invariant to significantly prune the undesired candidates and to pick out elliptical ones. The invariant is able to reflect the intrinsic geometry of a planar curve, giving the value of -1 on any three collinear points and +1 for any six points on an ellipse. Thus, we apply the pruning and picking by simply comparing these binary values. Moreover, the calculation of the invariant only involves the determinant of a 3*3 matrix. Extensive experiments on three challenging data sets with 650 images demonstrate that our detector runs 20%-50% faster than the state-of-the-art algorithms with the comparable or higher precision.", "url": "http://arxiv.org/abs/1608.07470"}
{"title": "[1608.07411] An Octree-Based Approach towards Efficient Variational Range Data Fusion", "sec_subject": "cs.CV", "num": 5, "authors": ["Wadim Kehl", "Tobias Holl", "Federico Tombari", "Slobodan Ilic", "Nassir Navab"], "order_index": 7565, "abstract": "Volume-based reconstruction is usually expensive both in terms of memory consumption and runtime. Especially for sparse geometric structures, volumetric representations produce a huge computational overhead. We present an efficient way to fuse range data via a variational Octree-based minimization approach by taking the actual range data geometry into account. We transform the data into Octree-based truncated signed distance fields and show how the optimization can be conducted on the newly created structures. The main challenge is to uphold speed and a low memory footprint without sacrificing the solutions' accuracy during optimization. We explain how to dynamically adjust the optimizer's geometric structure via joining/splitting of Octree nodes and how to define the operators. We evaluate on various datasets and outline the suitability in terms of performance and geometric accuracy.", "url": "http://arxiv.org/abs/1608.07411"}
{"title": "[1608.07338] Fast Trajectory Simplification Algorithm for Natural User Interfaces in Robot Programming by Demonstration", "sec_subject": "cs.CV", "num": 7, "authors": ["Daniel L. Marino", "Milos Manic"], "order_index": 7567, "abstract": "Trajectory simplification is a problem encountered in areas like Robot programming by demonstration, CAD/CAM, computer vision, and in GPS-based applications like traffic analysis. This problem entails reduction of the points in a given trajectory while keeping the relevant points which preserve important information. The benefits include storage reduction, computational expense, while making data more manageable. Common techniques formulate a minimization problem to be solved, where the solution is found iteratively under some error metric, which causes the algorithms to work in super-linear time. We present an algorithm called FastSTray, which selects the relevant points in the trajectory in linear time by following an open loop heuristic approach. While most current trajectory simplification algorithms are tailored for GPS trajectories, our approach focuses on smooth trajectories for robot programming by demonstration recorded using motion capture systems.Two variations of the algorithm are presented: 1. aims to preserve shape and temporal information; 2. preserves only shape information. Using the points in the simplified trajectory we use cubic splines to interpolate between these points and recreate the original trajectory. The presented algorithm was tested on trajectories recorded from a hand-tracking system. It was able to eliminate about 90% of the points in the original trajectories while maintaining errors between 0.78-2cm which corresponds to 1%-2.4% relative error with respect to the bounding box of the trajectories.", "url": "http://arxiv.org/abs/1608.07338"}
